OPENAI_EXCEPTION = "* Open LLM Leaderboard 2 scores are not availble for OpenAI models. MMLU-PRO scores for GPT-4o-2024-08-06 and GPT-4o-mini-2024-07-18 are obtained from [MMLU-PRO leaderboard](https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro). There might be differences in the scoring methodology."

ABOUT_SEAHELM = "SEA-HELM (SouthEast Asian Holistic Evaluation of Language Models) is an assessment of large language models across various tasks, emphasizing Southeast Asian languages. The leaderboard evaluates four key multilingual capabilities for instruction-tuned models: performance on the Open LLM Leaderboard v2 in English, proficiency in Southeast Asian chat, instruction-following in Southeast Asian languages, and Southeast Asian linguistic tasks. For base models, it assesses performance on the Open LLM Leaderboard v2 in English and proficiency in Southeast Asian linguistic tasks."

TASK_DETAILS = [
    "* Sentiment Analysis (Sentiment), Toxicity Detection (Toxicity), Causal Reasoning (Causal), and Natural Language Inference (NLI) tasks are evaluated using the macro-F1 metric.",
    "* Extractive Question Answering (QA) is evaluated using the F1 metric.",
    "* Translation tasks in both directions (Eng>Lang, Lang>Eng) are evaluated using the ChrF++ score.",
    "* Abstractive Summarization (Summarization) is evaluated using the ROUGE-L F1 score.",
    "* LINDSEA is a linguistic diagnostic dataset for SEA languages (only Indonesian and Tamil at this point). The score shown here is the average accuracy across the categories of syntax and pragmatics.",
]

ADDITIONAL_INFORMATION = [
    "* Open LLM Leaderboard v2 provides a comprehensive evaluation of large language models across a variety of challenging tasks, testing their reasoning, commonsense inference, multitask accuracy, truthfulness, and mathematical problem-solving abilities. This leaderboard offers a holistic view of model performance in diverse and demanding scenarios. More details can be found on HuggingFace [here](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) and task documentation can be found [here](https://huggingface.co/docs/leaderboards/open_llm_leaderboard/about).",
    "* SEA-MTBench evaluates a model's ability to engage in multi-turn (2 turns) conversations and respond in ways that align with human needs. We use gpt-4-1106-preview as the judge model and compare against gpt-3.5-turbo-0125 as the baseline model. The metric used is the weighted win rate against the baseline model (i.e. average win rate across each category (Math, Reasoning, STEM, Humanities, Roleplay, Writing, Extraction)). For Thai, SEA-HELM uses [MT-Bench Thai](https://huggingface.co/datasets/ThaiLLM-Leaderboard/mt-bench-thai) from the [ThaiLLM leaderboard](https://huggingface.co/ThaiLLM-Leaderboard).",
    "* SEA-IFEval evaluates a model's ability to adhere to constraints provided in the prompt, for example beginning a response with a specific word/phrase or answering with a certain number of sections. The metric used is accuracy normalized by language (if the model performs the task correctly but responds in the wrong language, it is judged to have failed the task).",
    "* BHASA (Benchmark for Holistic Evaluation of Generative AI in SoutheastAsian Languages) is a Southeast Asian evaluation benchmark built by AI Singapore for the evaluation of general multilingual capabilities. These tasks, available in various SEA languages, include Question Answering (QA), Sentiment Analysis (Sentiment), Toxicity Detection (Toxicity), Translation in both directions (Eng>Lang & Lang>Eng), Abstractive Summarization (Summ), Causal Reasoning (Causal) and Natural Language Inference (NLI). More details can be found on ArXiv [here](https://arxiv.org/abs/2309.06085).",
]

SCORE_CALCULATION = [
    "* Sentiment: Models must identify the sentiment of a particular piece of text (positive, negative, or neutral). Score calculation: We use weighted accuracy to account for class imbalance. The accuracy is normalized by subtracting the random baseline score and scaling it to a range of 0-100.",
    "* QA (Question Answering): Models are required to extract answers from a given text. Score calculation: We use the F1 score of the extracted answer against the reference answer. The F1 score is normalized by subtracting the random baseline score of 0 (as random guessing is unlikely) and scaling it to a range of 0-100.",
    "* Toxicity: Models must identify the toxicity of a particular piece of text (toxic, hateful, or non-toxic). Score calculation: We use weighted accuracy to account for class imbalance. The accuracy is normalized by subtracting the random baseline score and scaling it to a range of 0-100.",
    "* Metaphor (Indonesian Only): Models must identify which one of the two options best captures the meaning of a given metaphor. Score calculation: We use weighted accuracy to account for class imbalance. The accuracy is normalized by subtracting the random baseline score and scaling it to a range of 0-100.",
    "* Translation (Eng>Lang & Lang>Eng): Models are required to translate either an Indonesian, Vietnamese, Thai or Tamil sentence into English or vice versa. Score calculation: We use the chrF++ score of the translated answer against the reference text. The chrF++ score is normalized by subtracting the random baseline score of 0 (as random guessing is unlikely) and scaling it to a range of 0-100.",
    "* Summarization: Models are required to summarize a given Indonesian, Vietnamese, Thai or Tamil text. Score calculation: We use the F1 score of the extracted answer against the reference answer. The F1 score is normalized by subtracting the random baseline score of 0 (as random guessing is unlikely) and scaling it to a range of 0-100.",
    "* Causal (Causal Reasoning): Models are tested on whether they can correctly determine the relationship between the premise sentence and two given options using a “cause” or “effect” label as a hint. Score calculation: We use weighted accuracy to account for class imbalance. The accuracy is normalized by subtracting the random baseline score and scaling it to a range of 0-100.",
    "* NLI (Natural Language Inference): Models are tested on whether they can infer logical relationships between sentences X and Y (entailment, contradiction or no relation). Score calculation: We use weighted accuracy to account for class imbalance. The accuracy is normalized by subtracting the random baseline score and scaling it to a range of 0-100.",
    "* LINDSEA: Includes a diverse set of tasks that are designed to evaluate different aspects of models’ understanding of language. See our BHASA paper for more details. Score calculation: We use weighted accuracy to account for class imbalance. The accuracy is normalized by subtracting the random baseline score and scaling it to a range of 0-100.",
    "* Instruction Following (SEA-IFEval): Models are given instructions with specific constraints that they must adhere to in their responses. Score calculation: We use language-normalized rate of following specific constraints in instructions. The score is normalized by subtracting the random baseline score of 0 (as random guessing is unlikely) and scaling it to a range of 0-100.",
    "* Multi-Turn Chat (SEA-MTBench): Models receive an initial instruction based on topics ranging from math to creative writing that they must respond appropriately to, then given a second follow-up instruction that they must also respond appropriately to while taking into consideration the previous response as well. Score calculation: We use win rate against responses from gpt-3.5-turbo using GPT4-as-a-judge. The win rate is normalized by subtracting the random baseline score of 0 (as random guessing is unlikely) and scaling it to a range of 0-100.",
]

ABOUT_AISG = [
    "* AI Singapore, launched in May 2017, brings together all Singapore-based research institutions and the vibrant ecosystem of AI start-ups and companies developing AI products to perform use-inspired research, grow knowledge, create tools, and develop the talent to power Singapore’s AI efforts.",
    "* SEA-LION (Southeast Asian Languages In One Network) is a collection of Large Language Models (LLMs) which has been pretrained and instruct-tuned for the Southeast Asia (SEA) region. Find our latest models on [Huggingface](https://huggingface.co/aisingapore).",
    "* AI Singapore is committed to open-sourcing all artifacts for the benefit of Southeast Asia. We encourage researchers, developers, and language enthusiasts to actively contribute to the enhancement and expansion of SEA-LION. Contributions can involve identifying and reporting bugs, sharing pre-training, instruction, and preference data, improving documentation usability, proposing and implementing new model evaluation tasks and metrics, or training versions of the model in additional Southeast Asian languages.",
    "* Lastly, thank you for visiting us, we are glad to have you here! If you have any questions or feedback, please feel free to reach out to us [here](https://forms.gle/sLCUVb95wmGf43hi6). Join us in shaping the future of SEA-LION by sharing your expertise and insights to make these models more accessible, accurate, and versatile. Please check out our [GitHub](https://github.com/aisingapore/sealion) for further information.",
]
